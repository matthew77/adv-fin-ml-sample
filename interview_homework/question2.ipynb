{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## 问题回答\n",
    "\n",
    "如果某个分类特征由 N 个数值特征组合，应使用分类特征还是涵盖更多信息的全部 N 个数值特征（N 较小） ？ 例如： 情况 1： 使用 A, B,\n",
    "C 三个因子； 情况 2： 使用 A*(B>C) 因子（无需代码， 只需说明）\n",
    "\n",
    "回答：我倾向于两种情况同时使用。合成一个新的特征，如果新合成的特征对分类有更强的解释性，例如，房子的特征\n",
    "包含了L（长），W（宽）， 但是面积（S=W*L）可能是一个更好的特征，可以将L,W,S 这3个特征都加入模型进行训练。\n",
    "或者，在训练之前，进行特征选择，保留有意义的特征。\n",
    "\n",
    "## 模型部分\n",
    "\n",
    "### 数据分析/清理\n",
    "\n",
    "数据特征：\n",
    "- 不均衡，TargetFeature = 1 只占1%左右\n",
    "- 数据本身是time series，因此具有明显的auto correlation\n",
    "- 有大量缺失数据\n",
    "- 需要对inf/-inf进行特殊处理\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import talib\n",
    "from random import randrange\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_HOME = \"E:\\\\tmp\\\\data\\\\interview\"  # ！！！需要根据本地环境作修改\n",
    "LABELS = (\"Stock_1\", \"Stock_2\", \"Stock_3\", \"Stock_4\", \"Stock_5\", \"Stock_6\", \"Stock_7\", \n",
    "                \"Stock_8\", \"Stock_9\", \"Stock_10\")\n",
    "LOOK_BACK_PERIOD = [1,3,5,8,13,21,34]  # 回看时间长度\n",
    "SAMPLE_INTERVAL = [3, 15]  # 数据采样间隔\n",
    "ROLLING_WINDOW = 60    #计算滑动平均窗口长度\n",
    "\n",
    "    # ----------------------- clean data and create features -------------------\n",
    "def preprocess_data(label):\n",
    "    file_path = os.path.join(DATA_HOME, f'{label}.csv')\n",
    "    raw_data = pd.read_csv(file_path, index_col=0)\n",
    "    # 处理NaN单元格\n",
    "    raw_data.fillna(method='ffill', inplace=True)\n",
    "    raw_data.fillna(method='bfill', inplace=True)\n",
    "    raw_data.replace([np.inf, -np.inf], -1.0)   # not sure whether it is appropriate.\n",
    "    # 构建额外feature\n",
    "    features_for_training = dict()\n",
    "    ######################## price #######################\n",
    "    # mean price of feature 1,2,3,4 \n",
    "    raw_data['PriceFeature_mean']=raw_data[['PriceFeature_1','PriceFeature_2',\n",
    "                            'PriceFeature_3','PriceFeature_4']].mean(axis=1)  # 4者平均数\n",
    "    # bband ***\n",
    "    bb_up, bb_mid, bb_low = talib.BBANDS(raw_data['PriceFeature_mean'], timeperiod=ROLLING_WINDOW)\n",
    "    raw_data['new_price_bb_from_up'] = raw_data['PriceFeature_mean']/bb_up - 1\n",
    "    features_for_training['new_price_bb_from_up'] = 'float64'\n",
    "    raw_data['new_price_bb_from_low'] = raw_data['PriceFeature_mean']/bb_low - 1\n",
    "    features_for_training['new_price_bb_from_low'] = 'float64'\n",
    "    # gap between the price_feature 1 to 4\n",
    "    raw_data['new_price_gap_1m2'] = (raw_data['PriceFeature_1']-raw_data['PriceFeature_2'])/raw_data['PriceFeature_mean']\n",
    "    raw_data['new_price_gap_1m3'] = (raw_data['PriceFeature_1']-raw_data['PriceFeature_3'])/raw_data['PriceFeature_mean']\n",
    "    raw_data['new_price_gap_1m4'] = (raw_data['PriceFeature_1']-raw_data['PriceFeature_4'])/raw_data['PriceFeature_mean']\n",
    "    raw_data['new_price_gap_2m3'] = (raw_data['PriceFeature_2']-raw_data['PriceFeature_3'])/raw_data['PriceFeature_mean']\n",
    "    raw_data['new_price_gap_2m4'] = (raw_data['PriceFeature_2']-raw_data['PriceFeature_4'])/raw_data['PriceFeature_mean']\n",
    "    raw_data['new_price_gap_3m4'] = (raw_data['PriceFeature_3']-raw_data['PriceFeature_4'])/raw_data['PriceFeature_mean']\n",
    "\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_price_bb_from_up_t{i}'] = raw_data['new_price_bb_from_up'].shift(i)\n",
    "        features_for_training[f'new_price_bb_from_up_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_bb_from_low_t{i}'] = raw_data['new_price_bb_from_low'].shift(i)\n",
    "        features_for_training[f'new_price_bb_from_low_t{i}'] = 'float64'\n",
    "\n",
    "        raw_data[f'new_price_gap_1m2_t{i}'] = raw_data['new_price_gap_1m2'].shift(i)\n",
    "        features_for_training[f'new_price_gap_1m2_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_gap_1m3_t{i}'] = raw_data['new_price_gap_1m3'].shift(i)\n",
    "        features_for_training[f'new_price_gap_1m3_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_gap_1m4_t{i}'] = raw_data['new_price_gap_1m4'].shift(i)\n",
    "        features_for_training[f'new_price_gap_1m4_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_gap_2m3_t{i}'] = raw_data['new_price_gap_2m3'].shift(i)\n",
    "        features_for_training[f'new_price_gap_2m3_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_gap_2m4_t{i}'] = raw_data['new_price_gap_2m4'].shift(i)\n",
    "        features_for_training[f'new_price_gap_2m4_t{i}'] = 'float64'\n",
    "        raw_data[f'new_price_gap_3m4_t{i}'] = raw_data['new_price_gap_3m4'].shift(i)\n",
    "        features_for_training[f'new_price_gap_3m4_t{i}'] = 'float64'\n",
    "\n",
    "    # price momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        if i == 1:\n",
    "            continue  # it's just one period return\n",
    "        raw_data[f'new_price_mom_t{i}'] = raw_data['PriceFeature_mean'].pct_change(periods=i)\n",
    "        features_for_training[f'new_price_mom_t{i}'] = 'float64'\n",
    "    # log return ***\n",
    "    raw_data['new_price_log_rt'] = np.log(raw_data['PriceFeature_mean']).diff()\n",
    "    features_for_training['new_price_log_rt'] = 'float64'\n",
    "    ## mean price change and look back ***\n",
    "    ### log return look back ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_log_return_t{i}'] = raw_data['new_price_log_rt'].shift(i)\n",
    "        features_for_training[f'new_log_return_t{i}'] = 'float64'\n",
    "    ### PriceFeature_5 look back ***\n",
    "    features_for_training['PriceFeature_5'] = 'float64'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_price_5_t{i}'] = raw_data['PriceFeature_5'].shift(i)\n",
    "        features_for_training[f'new_price_5_t{i}'] = 'float64'\n",
    "    ### PriceFeature_6 look back ***\n",
    "    features_for_training['PriceFeature_6'] = 'float64'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_price_6_t{i}'] = raw_data['PriceFeature_6'].shift(i)\n",
    "        features_for_training[f'new_price_6_t{i}'] = 'float64'\n",
    "    ### PriceFeature_7 look back ***\n",
    "    features_for_training['PriceFeature_7'] = 'bool'\n",
    "    for i in LOOK_BACK_PERIOD :\n",
    "        raw_data[f'new_price_7_t{i}'] = raw_data['PriceFeature_7'].shift(i)\n",
    "        features_for_training[f'new_price_7_t{i}'] = 'float64'\n",
    "    ### PriceFeature_8 look back ***\n",
    "    features_for_training['PriceFeature_8'] = 'bool'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_price_8_t{i}'] = raw_data['PriceFeature_8'].shift(i)\n",
    "        features_for_training[f'new_price_8_t{i}'] = 'float64'\n",
    "\n",
    "    ######################## volume #######################\n",
    "    raw_data['new_vol_1_rolling_mean'] = raw_data['VolumeFeature_1'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    # *** vol norm + look back\n",
    "    raw_data['new_vol_1_norm'] = raw_data['VolumeFeature_1']/raw_data['new_vol_1_rolling_mean'] \n",
    "    features_for_training['new_vol_1_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_1_mom_t{i}'] = raw_data['new_vol_1_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_1_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_1_norm_t{i}'] = raw_data['new_vol_1_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_1_norm_t{i}'] = 'float64'\n",
    "\n",
    "    features_for_training['VolumeFeature_2'] = 'bool'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_2_t{i}'] = raw_data['VolumeFeature_2'].shift(i)\n",
    "        features_for_training[f'new_vol_2_t{i}'] = 'bool'\n",
    "\n",
    "    raw_data['new_vol_3_rolling_mean'] = raw_data['VolumeFeature_3'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    # *** vol norm + look back\n",
    "    raw_data['new_vol_3_norm'] = raw_data['VolumeFeature_3']/raw_data['new_vol_3_rolling_mean'] \n",
    "    features_for_training['new_vol_3_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_3_mom_t{i}'] = raw_data['new_vol_3_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_3_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_3_norm_t{i}'] = raw_data['new_vol_3_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_3_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_4_rolling_mean'] = raw_data['VolumeFeature_4'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_4_norm'] = raw_data['VolumeFeature_4']/raw_data['new_vol_4_rolling_mean']\n",
    "    features_for_training['new_vol_4_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_4_mom_t{i}'] = raw_data['new_vol_4_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_4_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_4_norm_t{i}'] = raw_data['new_vol_4_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_4_norm_t{i}'] = 'float64'\n",
    "        \n",
    "    raw_data['new_vol_5_rolling_mean'] = raw_data['VolumeFeature_5'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_5_norm'] = raw_data['VolumeFeature_5']/raw_data['new_vol_5_rolling_mean']\n",
    "    features_for_training['new_vol_5_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_5_mom_t{i}'] = raw_data['new_vol_5_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_5_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_5_norm_t{i}'] = raw_data['new_vol_5_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_5_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_6_rolling_mean'] = raw_data['VolumeFeature_6'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_6_norm'] = raw_data['VolumeFeature_6']/raw_data['new_vol_6_rolling_mean']\n",
    "    features_for_training['new_vol_6_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_6_mom_t{i}'] = raw_data['new_vol_6_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_6_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_6_norm_t{i}'] = raw_data['new_vol_6_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_6_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_7_rolling_mean'] = raw_data['VolumeFeature_7'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_7_norm'] = raw_data['VolumeFeature_7']/raw_data['new_vol_7_rolling_mean']\n",
    "    features_for_training['new_vol_7_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_7_mom_t{i}'] = raw_data['new_vol_7_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_7_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_7_norm_t{i}'] = raw_data['new_vol_7_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_7_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_8_rolling_mean'] = raw_data['VolumeFeature_8'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_8_norm'] = raw_data['VolumeFeature_8']/raw_data['new_vol_8_rolling_mean']\n",
    "    features_for_training['new_vol_8_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_8_mom_t{i}'] = raw_data['new_vol_8_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_8_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_8_norm_t{i}'] = raw_data['new_vol_8_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_8_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_9_rolling_mean'] = raw_data['VolumeFeature_9'].rolling(ROLLING_WINDOW).mean() + 0.0001 # mean can be 0!\n",
    "    raw_data['new_vol_9_norm'] = raw_data['VolumeFeature_9']/raw_data['new_vol_9_rolling_mean']\n",
    "    features_for_training['new_vol_9_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_9_mom_t{i}'] = raw_data['new_vol_9_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_9_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_9_norm_t{i}'] = raw_data['new_vol_9_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_9_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_10_rolling_mean'] = raw_data['VolumeFeature_10'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_10_norm'] = raw_data['VolumeFeature_10']/raw_data['new_vol_10_rolling_mean']\n",
    "    features_for_training['new_vol_10_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_10_mom_t{i}'] = raw_data['new_vol_10_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_10_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_10_norm_t{i}'] = raw_data['new_vol_10_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_10_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_11_rolling_mean'] = raw_data['VolumeFeature_11'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_11_norm'] = raw_data['VolumeFeature_11']/raw_data['new_vol_11_rolling_mean']\n",
    "    features_for_training['new_vol_11_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_11_mom_t{i}'] = raw_data['new_vol_11_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_11_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_11_norm_t{i}'] = raw_data['new_vol_11_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_11_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_vol_12_rolling_mean'] = raw_data['VolumeFeature_12'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_vol_12_norm'] = raw_data['VolumeFeature_12']/raw_data['new_vol_12_rolling_mean']\n",
    "    features_for_training['new_vol_12_norm'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_12_mom_t{i}'] = raw_data['new_vol_12_norm'].diff(periods=i)\n",
    "        features_for_training[f'new_vol_12_mom_t{i}'] = 'float64'\n",
    "        raw_data[f'new_vol_12_norm_t{i}'] = raw_data['new_vol_12_norm'].shift(i)\n",
    "        features_for_training[f'new_vol_12_norm_t{i}'] = 'float64'\n",
    "\n",
    "    features_for_training['VolumeFeature_13'] = 'float64'\n",
    "    ## vol Momentum ***\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_vol_13_t{i}'] = raw_data['VolumeFeature_13'].shift(i)\n",
    "        features_for_training[f'new_vol_13_t{i}'] = 'float64'        \n",
    "\n",
    "    ####################### Tape #######################\n",
    "    features_for_training['TapeFeature_1'] = 'bool'\n",
    "    features_for_training['TapeFeature_2'] = 'float64'\n",
    "    features_for_training['TapeFeature_4'] = 'float64'\n",
    "    features_for_training['TapeFeature_5'] = 'bool'\n",
    "    features_for_training['TapeFeature_6'] = 'float64'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_tape_1_t{i}'] = raw_data['TapeFeature_1'].shift(i)\n",
    "        features_for_training[f'new_tape_1_t{i}'] = 'bool'\n",
    "        raw_data[f'new_tape_2_t{i}'] = raw_data['TapeFeature_2'].shift(i)\n",
    "        features_for_training[f'new_tape_2_t{i}'] = 'float64'\n",
    "        raw_data[f'new_tape_4_t{i}'] = raw_data['TapeFeature_4'].shift(i)\n",
    "        features_for_training[f'new_tape_4_t{i}'] = 'float64'\n",
    "        raw_data[f'new_tape_5_t{i}'] = raw_data['TapeFeature_5'].shift(i)\n",
    "        features_for_training[f'new_tape_5_t{i}'] = 'bool'\n",
    "        raw_data[f'new_tape_6_t{i}'] = raw_data['TapeFeature_6'].shift(i)\n",
    "        features_for_training[f'new_tape_6_t{i}'] = 'float64'\n",
    "    \n",
    "    raw_data['new_tape_3_rolloing_abs_mean'] = raw_data['TapeFeature_3'].abs().rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    # normalize ***\n",
    "    raw_data['new_tape_3_abs_norm'] = raw_data['TapeFeature_3']/raw_data['new_tape_3_rolloing_abs_mean'] \n",
    "    features_for_training['new_tape_3_abs_norm'] = 'float64'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_tape_3_abs_norm_t{i}'] = raw_data['new_tape_3_abs_norm'].shift(i)\n",
    "        features_for_training[f'new_tape_3_abs_norm_t{i}'] = 'float64'\n",
    "\n",
    "    raw_data['new_tape_3_rolloing_mean'] = raw_data['TapeFeature_3'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    # normalize ***\n",
    "    raw_data['new_tape_3_norm'] = raw_data['TapeFeature_3']/raw_data['new_tape_3_rolloing_mean'] \n",
    "    features_for_training['new_tape_3_norm'] = 'float64'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_tape_3_norm_t{i}'] = raw_data['new_tape_3_abs_norm'].shift(i)\n",
    "        features_for_training[f'new_tape_3_norm_t{i}'] = 'float64'    \n",
    "\n",
    "    ####################### Transaction #######################\n",
    "    # look back\n",
    "    features_for_training['TransactionFeature_1'] = 'bool'\n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_trans_1_t{i}'] = raw_data['TransactionFeature_1'].shift(i)\n",
    "        features_for_training[f'new_trans_1_t{i}'] = 'bool'\n",
    "\n",
    "    ####################### order #######################\n",
    "    # normalize \n",
    "    raw_data['new_order_1_rolloing_mean'] = raw_data['OrderFeature_1'].rolling(ROLLING_WINDOW).mean() + 0.0001\n",
    "    raw_data['new_order_1_norm'] = raw_data['OrderFeature_1']/raw_data['new_order_1_rolloing_mean'] \n",
    "    features_for_training['new_order_1_norm'] = 'float64' \n",
    "    features_for_training['OrderFeature_2'] = 'float64' \n",
    "    features_for_training['OrderFeature_3'] = 'float64' \n",
    "    features_for_training['OrderFeature_4'] = 'float64' \n",
    "    for i in LOOK_BACK_PERIOD:\n",
    "        raw_data[f'new_order_1_norm_t{i}'] = raw_data['new_order_1_norm'].shift(i)\n",
    "        features_for_training[f'new_order_1_norm_t{i}'] = 'float64'\n",
    "        raw_data[f'new_order_2_t{i}'] = raw_data['OrderFeature_2'].shift(i)\n",
    "        features_for_training[f'new_order_2_t{i}'] = 'float64'\n",
    "        raw_data[f'new_order_3_t{i}'] = raw_data['OrderFeature_3'].shift(i)\n",
    "        features_for_training[f'new_order_3_t{i}'] = 'float64'\n",
    "        raw_data[f'new_order_4_t{i}'] = raw_data['OrderFeature_4'].shift(i)\n",
    "        features_for_training[f'new_order_4_t{i}'] = 'float64'\n",
    "    return raw_data, features_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据采样\n",
    "\n",
    "由于time series 数据具有auto correlation的特性，那么，相邻的数据具有很高的相关性。因此，如果直接使用这些数据，那么这些数据\n",
    "很明显就不是IID，这样训练出来的模型效果会受到影响。\n",
    "\n",
    "这里使用一种简单的方法，就是随机间隔采样，用这种方式对负样本进行采样，同时完整保留正样本。因为，正样本实在太少了，因此受auto correlation\n",
    "的影响可以忽略。\n",
    "\n",
    "强调：这里的目的不是欠采样，而是希望训练的数据更加符合IID以提高训练质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(raw_data):\n",
    "    df = raw_data.dropna()\n",
    "    \n",
    "    first_index = df.iloc[[0,-1]].index[0]\n",
    "    last_index = df.iloc[[0,-1]].index[1]\n",
    "    sample_ids = set()\n",
    "    # keep all TargetFeature = 1\n",
    "    for i in df[df['TargetFeature']==1].index:\n",
    "        sample_ids.add(i)\n",
    "        # also get the neighbor\n",
    "        if i - 1 >= first_index:\n",
    "            sample_ids.add(i - 1)\n",
    "        if i + 1 <= last_index:\n",
    "            sample_ids.add(i + 1)\n",
    "    row_index = first_index - SAMPLE_INTERVAL[0]  # in accordance with randrange(3, 15)\n",
    "    while True:\n",
    "        row_index += randrange(SAMPLE_INTERVAL[0], SAMPLE_INTERVAL[1])  # uniform distribution between 3~15\n",
    "        if row_index <= last_index:\n",
    "            sample_ids.add(row_index)\n",
    "        else:\n",
    "            break\n",
    "    sample_ids = sorted(sample_ids)\n",
    "    return df.reindex(sample_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "\n",
    "这里通过两种方式进行特征筛选，获取两者的并集。两者方式分别是DecisionTreeClassifier以及RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将stock1~10的数据汇总\n",
    "total_data = pd.DataFrame()\n",
    "for label in LABELS:\n",
    "    print(f'processing {label} file ......')\n",
    "    data, features = preprocess_data(label)\n",
    "    sample_data = sampling(data)\n",
    "    sample_data['label'] = label  # add a new column/feature\n",
    "    total_data = total_data.append(sample_data.copy())    \n",
    "total_data.replace([np.inf, -np.inf], -1.0, inplace=True)\n",
    "total_data.dropna()\n",
    "\n",
    "# use DecisionTreeClassifier\n",
    "df_feature_sel = total_data[total_data[\"label\"] == 'Stock_1']\n",
    "X = df_feature_sel[list(features.keys())]\n",
    "# X = X.replace([np.inf, -np.inf], -1.0)   # ???How to handle inf??? # !!! to be deleted\n",
    "y = df_feature_sel['TargetFeature']\n",
    "clf = DecisionTreeClassifier(class_weight='balanced')  # consider the imbalance training data\n",
    "trans = SelectFromModel(clf, threshold='0.05*mean')  #, threshold='0.1*mean'\n",
    "X_trans = trans.fit_transform(X, y)\n",
    "print(\"We started with {0} features but retained only {1} of them!\".format(X.shape[1], X_trans.shape[1]))\n",
    "useful_featrues_1 = X.columns[trans.get_support()].values\n",
    "\n",
    "# use RandomForestClassifier\n",
    "le = LabelEncoder()\n",
    "# X = total_data.replace([np.inf, -np.inf], -1.0) # !!! to be deleted\n",
    "total_data[\"label_encode\"] = le.fit_transform(total_data[\"label\"].values)\n",
    "feature_list = list(features.keys())\n",
    "feature_list.append('label_encode')  # Stock_1 ... 10 is a feature.\n",
    "X = total_data[feature_list]\n",
    "y = total_data['TargetFeature']\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=0, class_weight='balanced')\n",
    "trans = SelectFromModel(clf, threshold='median') \n",
    "X_trans = trans.fit_transform(X, y)\n",
    "print(\"We started with {0} features but retained only {1} of them!\".format(X.shape[1], X_trans.shape[1]))\n",
    "useful_featrues_2 = X.columns[trans.get_support()].values\n",
    "# merge the features \n",
    "features_for_train = set(useful_featrues_1) | set(useful_featrues_2)\n",
    "features_for_train.add('label_encode')\n",
    "features_for_train = list(features_for_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "这里使用RandomForestClassifier， 需要注意，由于训练数据是非均衡的，因此需要设置class_weight的参数以适应这种场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3, shuffle=False) # 注意这里shuffle是false\n"
   ]
  }
 ]
}